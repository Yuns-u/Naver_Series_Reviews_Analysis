{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "한국어_text시도.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMMb5thGYVyi7Kfe9U5eAOO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yuns-u/Naver_Series_Reviews_Analysis/blob/main/%ED%95%9C%EA%B5%AD%EC%96%B4_text%EC%8B%9C%EB%8F%84.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWOXCPaqwd4p"
      },
      "source": [
        "# 필요한 모듈 불러오기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSRg-B3tFu23"
      },
      "source": [
        "pip install konlpy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lIyn-MGUF3tT"
      },
      "source": [
        "pip install scikit-learn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6YeAsbDZBmvD"
      },
      "source": [
        "from konlpy.tag import Kkma\n",
        "from konlpy.tag import Twitter\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.preprocessing import normalize\n",
        "import numpy as np\n",
        "from math import log # IDF 계산을 위해\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lu2ExmVUwcmv"
      },
      "source": [
        "# 전처리\n",
        "\n",
        "텍스트를 문장으로 이루어진 리스트로 만들고 불용어 등을 처리해주기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmtGWRgj2iGf"
      },
      "source": [
        "## 정규표현식으로 전처리하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TvRkaT_zg4QK"
      },
      "source": [
        "----"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gpj0_KMSX2UH"
      },
      "source": [
        "#데이터 불러오기\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "import pandas as pd\n",
        "df = pd.read_csv('iOS_train.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MnOL-8MAYKqQ"
      },
      "source": [
        "#불필요한 column 지우기\n",
        "df = df.drop(columns='Unnamed: 0')\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5GlETp0g5l1"
      },
      "source": [
        "import re \n",
        "\n",
        "#한글만 남기고 특수기호를 정규표현식을 데이터프레임에 적용해서 모두 없앰.\n",
        "#df['REVIEW']=df['REVIEW'].str.replace(pat=r'[^\\w]', repl= r' ', regex=True)\n",
        "#df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_6Fq_YdE3TF"
      },
      "source": [
        "def cleasing(text): \n",
        "  repl ='' \n",
        "  pattern = '([ㄱ-ㅎㅏ-ㅣ]+)' # 자음, 모음 제거 \n",
        "  text = re.sub(pattern= pattern, repl=repl, string=text) \n",
        "  #pattern = '[^\\w\\s]' # 특수기호 제거 \n",
        "  pattern = '[^가-히\\s]' # 특수기호 제거 \n",
        "  text = re.sub(pattern= pattern, repl=repl, string=text) \n",
        "  pattern = '<[^>]*>' # html 제거 \n",
        "  text = re.sub(pattern = pattern, repl='',string=text) \n",
        "  return text\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XRKIJzpUFEaE"
      },
      "source": [
        "df['REVIEW'] = df['REVIEW'].map(lambda x: cleasing(x))\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79L6-Rgy2AMt"
      },
      "source": [
        "# 토큰화\n",
        "\n",
        "한국어 토큰화를 도와주는 형태소 분석기는 konlpy의 Kkma, Okt, mecab 또는 Pykomoran 등이 있고 성능이 각각 다르다고 한다.\n",
        "\n",
        "- 1) morphs : 형태소 추출\n",
        "- 2) pos : 품사 태깅(Part-of-speech tagging)\n",
        "- 3) nouns : 명사 추출\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZyUFiCPc1XrT"
      },
      "source": [
        "from konlpy.tag import Okt  \n",
        "okt = Okt()\n",
        "\n",
        "print(okt.morphs(\"아이패드 앱에서 다크모드 및 스플릿뷰는 언제 지원될까요?\"))\n",
        "print(okt.pos(\"아이패드 앱에서 다크모드 및 스플릿뷰는 언제 지원될까요?\"))\n",
        "print(okt.nouns(\"아이패드 앱에서 다크모드 및 스플릿뷰는 언제 지원될까요?\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gg9971oQBtm2"
      },
      "source": [
        "kkma = Kkma()\n",
        "\n",
        "print(kkma.morphs(\"아이패드 앱에서 다크모드 및 스플릿뷰는 언제 지원될까요?\"))\n",
        "print(kkma.pos(\"아이패드 앱에서 다크모드 및 스플릿뷰는 언제 지원될까요?\"))\n",
        "print(kkma.nouns(\"아이패드 앱에서 다크모드 및 스플릿뷰는 언제 지원될까요?\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nOx5RUCXtTZz"
      },
      "source": [
        "키워드를 추출할 때에 많은 경우 명사를 조합하여 간략하게 표현했었다.\n",
        "명사를 추출하는 것만 해도 본 프로젝트의 목표를 달성하는 데에는 충분할 것 같다.\n",
        "또한 kkma에서는 외래어에 한국어가 들어가면 일단 한국어로 의미가 되는 부분을 나눈다는 것을 확인할 수 있었다. 어떤 모듈을 쓰느냐에 따라 성능의 차이가 클 것이라 생각된다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xt8vQJcvu66s"
      },
      "source": [
        "okt.nouns(\"웹툰이랑 소설 같이 볼 수 있어서 좋구 디자인도 깔끔해서 보기 좋아요!\\n아쉬운 점은 네이버 웹툰처럼 캡쳐할 수 있는 기능 달아주셨으면 좋겠어요ㅠㅠㅠ\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QST6jJwACMH-"
      },
      "source": [
        "kkma.nouns('다신 안 쓸거임! 고마워요😆!!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uwl89QElw1HF"
      },
      "source": [
        "okt.nouns('다신 안 쓸거임! 고마워요😆!!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFUecbyIxP-t"
      },
      "source": [
        "okt.pos('다신 안 쓸거임! 고마워요😆')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cKg9LD0xINF"
      },
      "source": [
        "하지만 부정적인 평가의 경우 위처럼 명사만 추출할 시(사실 명사라고 보기 애매하다..) 명사형 어간어미도 명사라고 보는 등\n",
        "정확도도 떨어지고 문맥을 알 수 없어 오히려 난감할 수 있다. 따라서 조사나 어말어미 등은 나중에 불용어처리로 지워주는 것이 좋을 것이라 생각한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVYGJtjtzBin"
      },
      "source": [
        "# 불용어 정의\n",
        "stopwords=['의','가','이','은','들','는','좀','잘','걍','과','도','를','을','으로','자','에','와','한','하다','요', '아니',\n",
        "           '해서','에서','제','신건','아니','저기','주세요','해','서','되는','논','여','저','로','으로','오','고','랑','이랑',\n",
        "           '합니다','부터','\\n','\\n\\n','왜','입니다','에도','하고','만','다','너무','했는데','수','것','때','거','하는','있는',\n",
        "           '하는']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_R22DbJrxCvr"
      },
      "source": [
        "tokenizer = Okt()\n",
        "\n",
        "tokenized=[]\n",
        "for sentence in df['REVIEW']:\n",
        "    temp = tokenizer.nouns(sentence) # 토큰화morphs를 쓸 때보다 명사만 추출했을 때 더 잘 파악됨. 한국어 표제어 추출이 어려움.\n",
        "    temp = [word for word in temp if not word in stopwords] # 불용어 제거\n",
        "    tokenized.append(temp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-t2dYHAzgLB"
      },
      "source": [
        "print(tokenized[:10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZuYtWA9uivL"
      },
      "source": [
        "df['tokens'] = tokenized\n",
        "df['tokens']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ikwHJ4A5D0q"
      },
      "source": [
        "# 리뷰 길이 분포 확인\n",
        "print('리뷰의 최대 길이 :',max(len(l) for l in tokenized))\n",
        "print('리뷰의 평균 길이 :',sum(map(len, tokenized))/len(tokenized))\n",
        "plt.hist([len(s) for s in tokenized], bins=50)\n",
        "plt.xlabel('length of samples')\n",
        "plt.ylabel('number of samples')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZDle7TwuKVU"
      },
      "source": [
        "from collections import Counter\n",
        "\n",
        "word_counted = Counter()\n",
        "df['tokens'].apply(lambda x: word_counted.update(x))\n",
        "\n",
        "#빈도 상위 토큰 보기\n",
        "#빈도를 보면서 불용어에 지속적으로 업데이트함.\n",
        "word_counted.most_common(30)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wWOdgkO7syHm"
      },
      "source": [
        "#토큰화된 문서들을 입력받아 토큰을 센 뒤 관련된 속성을 가진 데이터프레임을 return\n",
        "def word_count(docs):\n",
        "  #전체 코퍼스에서 단어 빈도 카운트\n",
        "  word_counts = Counter()\n",
        "\n",
        "  #단어가 존재하는 문서의 빈도 카운트, 단어가 한 번 이상 존재하면 1을 더한다.\n",
        "  word_in_docs = Counter()\n",
        "\n",
        "  #전체 문서의 개수\n",
        "  total_docs = len(docs)\n",
        "\n",
        "  for doc in docs:\n",
        "    word_counts.update(doc)\n",
        "    word_in_docs.update(set(doc))\n",
        "\n",
        "  temp = zip(word_counts.keys(), word_counts.values())\n",
        "  wc = pd.DataFrame(temp, columns=['word','count'])\n",
        "\n",
        "  #단어 순위\n",
        "  #method='first':같은 값의 경우 먼저 나온 요소를 우선\n",
        "  wc['rank'] = wc['count'].rank(method='first', ascending=False)\n",
        "  total = wc['count'].sum()\n",
        "\n",
        "  #코퍼스 내 단어의 비율\n",
        "  wc['percent'] = wc['count'].apply(lambda x: x/ total)\n",
        "  wc = wc.sort_values(by='rank')\n",
        "\n",
        "  #누적 비율\n",
        "  #cumsum() : cumulative sum\n",
        "  wc['cul_percent'] = wc['percent'].cumsum()\n",
        "\n",
        "  temp2 = zip(word_in_docs.keys(), word_in_docs.values())\n",
        "  ac = pd.DataFrame(temp2, columns=['word','word_in_docs'])\n",
        "  wc = ac.merge(wc, on='word')\n",
        "\n",
        "  #전체 문서 중 존재하는 비율\n",
        "  wc['word_in_docs_percent'] = wc['word_in_docs'].apply(lambda x: x/total_docs)\n",
        "\n",
        "  return wc.sort_values(by='rank')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0m-FlZasfBR"
      },
      "source": [
        "wc = word_count(df['tokens'])\n",
        "wc.head(20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZrDli3ghEHfo"
      },
      "source": [
        "wc.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHeHDfHm752t"
      },
      "source": [
        "#토큰 순위에 따른 퍼센트 누적 분포 그래프\n",
        "import seaborn as sns\n",
        "\n",
        "sns.lineplot(x='rank', y='cul_percent', data=wc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7beKPDarCf4"
      },
      "source": [
        "# Lemmetization 표제어 추출해보기\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VjD9DDXQrv6x"
      },
      "source": [
        "lem_df = df.copy()\n",
        "lem_df\n",
        "#한국어 표제어 추출은 일단 보류해야할 것 같다..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BqEbLFJVArtS"
      },
      "source": [
        "# tf-idf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SHZi-ecAB-2"
      },
      "source": [
        "tfodf_vector = TfidfVectorizer(min_df=1)\n",
        "tfidf_matrix = tfodf_vector.fit_transform(df['tokens'])\n",
        "\n",
        "doc_distance = (tfidf_matrix*tfidf_matrix.T)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9LgHCT5DWxG"
      },
      "source": [
        "# 키워드 추출\n",
        "KR-WordRank\n",
        "비지도학습 방법으로 한국어 텍스트에서 단어/키워드를 자동으로 추출하는 라이브러리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XEiuANiCDkZs"
      },
      "source": [
        "pip install krwordrank"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpavDk3BDWdD"
      },
      "source": [
        "from krwordrank.word import KRWordRank\n",
        "\n",
        "min_count = 3   # 단어의 최소 출현 빈도수 (그래프 생성 시)\n",
        "max_length = 10 # 단어의 최대 길이\n",
        "wordrank_extractor = KRWordRank(min_count=min_count, max_length=max_length)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9OZh9vVERmn"
      },
      "source": [
        "beta = 0.85    # PageRank의 decaying factor beta\n",
        "max_iter = 10\n",
        "texts = lem_df['REVIEW'].values\n",
        "keywords, rank, graph = wordrank_extractor.extract(texts, beta, max_iter)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXLHAbTiFtpP"
      },
      "source": [
        "Graph ranking 이 높은 노드들(substrings)이 후처리 과정을 거쳐 단어로 출력.\n",
        "\n",
        "키워드 (단어) 추출을 한 결과는 아래와 같다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oVTWQtBwFkBp"
      },
      "source": [
        "for word, r in sorted(keywords.items(), key=lambda x:x[1], reverse=True)[:30]:\n",
        "        print('%8s:\\t%.4f' % (word, r))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISSmL9houFeV"
      },
      "source": [
        "# Word2Vec\n",
        "\n",
        "단어를 벡터로 만드는 방법으로 가장 널리 사용되는 임베딩 방법.\n",
        "- CBoW: 주변 단어에 대한 정보를 기반으로 중심 단어의 정보를 예측하는 모델\n",
        "- Skip-gram: 중심 단어의 정보를 기반으로 주변 단어의 정보를 예측하는 모델*\n",
        "\n",
        "**Skip-gram**\n",
        "\n",
        "- 입력: 원핫인코딩된 단어벡터\n",
        "- 은닉: 임베딩 벡터의 차원수만큼의 노드로 구성된 은닉층이 1개인 신경망\n",
        "- 출력: 단어 개수만큼의 노드로 이루어져 있으며 활성화 함수로 소프트맥스 사용"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNgHv3YGuF2u"
      },
      "source": [
        "pip install gensim --upgrade"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WI-rAeO00WUY"
      },
      "source": [
        "from gensim.models.word2vec import Word2Vec\n",
        "from tqdm import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CpQf1cTf22z2"
      },
      "source": [
        "model = Word2Vec(sentences = tokenized, window = 5, min_count = 3, workers = 4, sg = 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KeamVRQ54vV9"
      },
      "source": [
        "# 완성된 임베딩 매트릭스의 크기 확인\n",
        "#493개의 단어이며 각 단어는 100차원으로 구성되어 있음.\n",
        "model.wv.vectors.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPkmJMVw4yGx"
      },
      "source": [
        "print(model.wv.most_similar(\"시리즈\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r8lYbeZL9CXw"
      },
      "source": [
        "print(model.wv.most_similar(\"쿠키\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGT3Djnh9FbT"
      },
      "source": [
        "print(model.wv.most_similar(\"무료\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IcG2gEoi_KwJ"
      },
      "source": [
        "문서 내 각 단어들을 Word2Vec을 통해 단어 벡터로 변환하고, 이들의 평균으로 문서 벡터를 구하여 선호하는 키워드와 유사한 키워드를 찾아주는 리뷰 키워드 추천 시스템"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AA8r0EZI_KiJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0BmODbnwR750"
      },
      "source": [
        "# fastText \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wkMNjdbwS3Dp"
      },
      "source": [
        "pip install fasttext"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j1UHbF1gSBBo"
      },
      "source": [
        "from pprint import pprint as print\n",
        "from gensim.models import fasttext\n",
        "#from gensim.test.utils import datapath\n",
        "from gensim import models\n",
        "\n",
        "ko_model = models.fasttext.load_facebook_model('/Users/yunsu/Downloads/wiki.ko/wiki.ko.vec')\n",
        "\n",
        "for w, sim in m_fasttext.similar_by_word('파이썬', 10):\n",
        "    print(f'{w}: {sim}')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}